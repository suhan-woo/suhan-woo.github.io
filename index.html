<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Suhan Woo</title>
  
  <meta name="author" content="Suhan Woo">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üèÇ</text></svg>">
  <!-- <link rel="icon" href="images/logo/homepage_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Suhan Woo</name>
              </p>
              <p>
                I am a Ph.D canditate at <a href="https://cilab.yonsei.ac.kr">CILab</a> in Yonsei university, <a href="https://en.wikipedia.org/wiki/Seoul">Seoul</a>, South Korea.
              </p>
              <p>
                My research mainly focuses on various 2D/3D computer vision tasks including generative models, and their applications to intelligent vehicle and robotic systems.
              </p>
              <p>
                I'm always open to collaborations or suggestions. Please feel free to contact me if you have any questions or suggestions. :)
              </p>
              <p style="text-align:center">
                <a href="mailto:wsh112@yonsei.ac.kr">Email</a> &nbsp/&nbsp
                <a href="data/resume/suhanwoo-resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/suhan-woo/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile/suhanwoo.png">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publication</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



          <tr onmouseout="latnet_stop()" onmouseover="latnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='latnet_image'>
                  <img src='images/latnet/latnet_after.png' width="160">
                </div>
                <img src='images/latnet/latnet_before.png' width="160">
              </div>
              <script type="text/javascript">
                function latnet_start() {
                  document.getElementById('latnet_image').style.opacity = "1";
                }

                function latnet_stop() {
                  document.getElementById('latnet_image').style.opacity = "0";
                }
                latnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10729649">
                <papertitle>Location-Aware Transformer Network for Bird‚Äôs Eye View Semantic Segmentation</papertitle>
              </a>
              <br>
              <strong>Suhan Woo</strong>,
              <a href="https://github.com/minseong-p">Minseong Park</a>,
              <a href="https://scholar.google.co.kr/citations?user=VxUl1wsAAAAJ&hl=en">Youngjo Lee</a>,
              <a href="https://sungonce.github.io">Seongwon Lee</a>,
              <a href="https://cilab.yonsei.ac.kr/">Euntai Kim</a>
              <br>
              <em> IEEE Transactions on Intelligent Vehicles, vol. 10, no. 9, pp. 4467‚Äì4478, Sep. 2025. </em>
              (IF: 14.3 in JCR2024)
              <br>

              <a href="https://ieeexplore.ieee.org/document/10729649">Paper</a>

              /
              <a href="data/bib/latnet_2024.txt">bib</a>
              <p></p>
              <p>
              We propose a novel BEV segmentation network that adaptively utilizes features of various scales based on the location in the BEV space. 
              </p>
            </td>
          </tr>

          <tr onmouseout="dnmap_stop()" onmouseover="dnmap_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dnmap_image'>
                  <img src='images/dnmap/dnmap_after.png' width="160">
                </div>
                <img src='images/dnmap/dnmap_before.png' width="160">
              </div>
              <script type="text/javascript">
                function dnmap_start() {
                  document.getElementById('dnmap_image').style.opacity = "1";
                }

                function dnmap_stop() {
                  document.getElementById('dnmap_image').style.opacity = "0";
                }
                dnmap_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/09144.pdf">
                <papertitle>Decomposition of Neural Discrete Representations for Large-Scale 3D Mapping</papertitle>
              </a>
              <br>
              <a href="https://github.com/minseong-p">Minseong Park</a>,
              <strong>Suhan Woo</strong>,
              <a href="https://cilab.yonsei.ac.kr/">Euntai Kim</a>
              <br>
              <em>European Conference on Computer Vision (ECCV)</em>, 2024
              (Acceptance Rate: 27.9%)
              <br>
              <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/09144.pdf">Paper</a>
              /
              <a href="https://github.com/minseong-p/dnmap">Code</a>
              /
              <a href="data/bib/dnmap_2024.txt">bib</a>
              <p></p>
              <p>
              We propose a storage-efficient large-scale 3D mapping method that employs a discrete representation based on a decomposition strategy. 
              </p>
            </td>
          </tr>



          <tr onmouseout="hypevpr_stop()" onmouseover="hypevpr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hypevpr_image'>
                  <img src='images/hypevpr/hypevpr_after.png' width="160">
                </div>
                <img src='images/hypevpr/hypevpr_before.png' width="160">
              </div>
              <script type="text/javascript">
                function hypevpr_start() {
                  document.getElementById('hypevpr_image').style.opacity = "1";
                }
          
                function hypevpr_stop() {
                  document.getElementById('hypevpr_image').style.opacity = "0";
                }
                hypevpr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>HypeVPR: Exploring Hyperbolic Space for Perspective to Equirectangular Visual Place Recognition </papertitle>
              </a>
              <br>
              <strong>Suhan Woo</strong>,
              <a href="https://sungonce.github.io">Seongwon Lee</a>,
              Jinwoo Jang,
              <a href="https://cilab.yonsei.ac.kr/">Euntai Kim</a>
              <br>
              <em> Under Review </em>
              <br>
              <a href="https://www.arxiv.org/abs/2506.04764">Paper</a>
                  /
              <a href="data/bib/hypevpr_2024.txt">bib</a>
              
              <p></p>
              <p>
              We propose a novel P2E VPR method that leverages the properties of hyperbolic space to address the matching problem between perspective views and equirectangular images. 
              </p>
            </td>
          </tr>


          <tr onmouseout="a2lc_stop()" onmouseover="a2lc_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='a2lc_image'>
                  <img src='images/a2lc/a2lc_after.png' width="160">
                </div>
                <img src='images/a2lc/a2lc_before.png' width="160">
              </div>
              <script type="text/javascript">
                function a2lc_start() {
                  document.getElementById('a2lc_image').style.opacity = "1";
                }

                function a2lc_stop() {
                  document.getElementById('a2lc_image').style.opacity = "0";
                }
                a2lc_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>A<sup>2</sup>LC: Active and Automated Label Correction for Semantic Segmentation. </papertitle>
              </a>
              <br>
              Youjin Jeon*,
              Kyusik Cho*,
              <strong>Suhan Woo</strong>,
              <a href="https://cilab.yonsei.ac.kr/">Euntai Kim </a>
              (* Equal contribution)
              <br>
              <em> AAAI Conference on Artificial Intelligence (AAAI-26) (Acceptance Rate: 17.6%)</em>
              <br>
              <a href="https://arxiv.org/abs/2506.11599">Paper</a>
                  /
              <a href="data/bib/a2lc_2025.txt">bib</a>
              
              <p></p>
              <p>
              We propose A2LC, a novel active and automated label correction framework for semantic segmentation that integrates automated correction guided by annotator feedback with adaptive sample acquisition, achieving significantly higher efficiency and performance than prior ALC methods. 
              </p>
            </td>
          </tr>


          <tr onmouseout="rgbd_segmentation_stop()" onmouseover="rgbd_segmentation_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rgbd_segmentation_image'>
                  <img src='images/rgbd_segmentation/rgbd_segmentation_after.png' width="160">
                </div>
                <img src='images/rgbd_segmentation/rgbd_segmentation_before.png' width="160">
              </div>
              <script type="text/javascript">
                function rgbd_segmentation_start() {
                  document.getElementById('rgbd_segmentation_image').style.opacity = "1";
                }

                function rgbd_segmentation_stop() {
                  document.getElementById('rgbd_segmentation_image').style.opacity = "0";
                }
                rgbd_segmentation_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Real-Time RGB-D Semantic Segmentation via Efficient Depth Encoding and Fusion</papertitle>
              </a>
              <br>
              <strong>Suhan Woo</strong>,
              <a href="https://cilab.yonsei.ac.kr/cv/cv_junhyukhyun.pdf">Junhyuk Hyun</a>,
              <a href="https://suhyeonlee.github.io/">Suhyeon Lee</a>,
              <a href="https://cilab.yonsei.ac.kr/">Euntai Kim</a>
              <br>
              <em> International Journal of Control, Automation, and Systems </em>, vol. 23 no. 12 (2025) pp. 3649-3661 
              (IF: 2.9 in JCR2024)
              <br>

              <p></p>
              <p>

            This paper proposes a real-time RGB-D semantic segmentation method that effectively encodes depth information and fuses it with RGB features to enhance segmentation performance while maintaining computational efficiency.
              </p>
            </td>
          </tr>


          <tr onmouseout="bridgeta_stop()" onmouseover="bridgeta_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bridgeta_image'>
                  <img src='images/bridgeta/bridgeta_after.png' width="160">
                </div>
                <img src='images/bridgeta/bridgeta_before.png' width="160">
              </div>
              <script type="text/javascript">
                function bridgeta_start() {
                  document.getElementById('bridgeta_image').style.opacity = "1";
                }

                function bridgeta_stop() {
                  document.getElementById('bridgeta_image').style.opacity = "0";
                }
                bridgeta_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird‚Äôs Eye View Map Segmentation</papertitle>
              </a>
              <br>
              Beomjun Kim,
              <strong>Suhan Woo</strong>,
              Sejong Heo,
              <a href="https://cilab.yonsei.ac.kr/">Euntai Kim</a>
              <br>
              <em> Under Review </em>
              <br>
              <a href="https://www.arxiv.org/abs/2508.09599">Paper</a>
                  /
              <a href="data/bib/bridgeta_2025.txt">bib</a>
              
              <p></p>
              <p>
              We propose BridgeTA, a cost-effective knowledge distillation framework that bridges the representation gap between LiDAR-Camera fusion and camera-only models through a lightweight teacher-assistant network, achieving superior efficiency and performance in BEV map segmentation.
              </p>
            </td>
          </tr>
          

                    <tr onmouseout="ecd_stop()" onmouseover="ecd_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ecd_image'>
                  <img src='images/ecd/ecd_after.png' width="160">
                </div>
                <img src='images/ecd/ecd_before.png' width="160">
              </div>
              <script type="text/javascript">
                function ecd_start() {
                  document.getElementById('ecd_image').style.opacity = "1";
                }

                function ecd_stop() {
                  document.getElementById('ecd_image').style.opacity = "0";
                }
                ecd_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Environmental Change Detection: Toward a Practical Task of Scene Change Detection </papertitle>
              </a>
              <br>
              Kyusik Cho,
              <strong>Suhan Woo</strong>,
              <a href="https://hongje.github.io">Hongje Seong</a>,
              <a href="https://cilab.yonsei.ac.kr/">Euntai Kim</a>
              <br>
              <em> Under Review </em>
              <br>
              <a href="https://arxiv.org/abs/2506.11481">Paper</a>
                  /
              <a href="data/bib/ecd_2025.txt">bib</a>
              
              <p></p>
              <p>
              We propose a novel Environmental Change Detection (ECD) framework that handles misaligned and uncurated reference images by aggregating multiple reference candidates and rich semantic cues, achieving robust and state-of-the-art performance on standard benchmarks.
              </p>
            </td>
          </tr>


 









<tr onmouseout="floor_segmentation_stop()" onmouseover="floor_segmentation_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='floor_segmentation_image'>
        <img src='images/floor_segmentation/floor_segmentation_after.png' width="160">
      </div>
      <img src='images/floor_segmentation/floor_segmentation_before.png' width="160">
    </div>
    <script type="text/javascript">
      function floor_segmentation_start() {
        document.getElementById('floor_segmentation_image').style.opacity = "1";
      }

      function floor_segmentation_stop() {
        document.getElementById('floor_segmentation_image').style.opacity = "0";
      }
      floor_segmentation_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">

    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9970739">
      <papertitle>Street Floor Segmentation for a Wheeled Mobile Robot</papertitle>
    </a>
    <br>
    <a href="https://cilab.yonsei.ac.kr/cv/cv_junhyukhyun.pdf">Junhyuk Hyun</a>,
    <strong>Suhan Woo</strong>,
    <a href="https://cilab.yonsei.ac.kr/">Euntai Kim</a>
    <br>

    <em>IEEE Access</em>, 2022
    (IF: 3.4 in JCR2023)
    <br>
    <a href="https://ieeexplore.ieee.org/abstract/document/9970739">Paper</a>
    /
    <a href="data/bib/floor_segmentation_2022.txt">bib</a>

    <p></p>
    <p>

    This paper proposes a real-time RGB-based street floor segmentation method that identifies traversable and non-traversable curbs to support mobile robot navigation in urban environments. 
    </p>
  </td>
</tr>

<tr onmouseout="three_D_deep_stop()" onmouseover="three_D_deep_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='deep_learning_image'>
        <img src='images/3d_deep/3d_deep_after.png' width="160">
      </div>
      <img src='images/3d_deep/3d_deep_before.png' width="160">
    </div>
    <script type="text/javascript">
      function three_D_deep_start() {
        document.getElementById('deep_learning_image').style.opacity = "1";
      }

      function three_D_deep_stop() {
        document.getElementById('deep_learning_image').style.opacity = "0";
      }
      three_D_deep_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ieeexplore.ieee.org/abstract/document/9304601">
      <papertitle>3D-DEEP: 3-Dimensional Deep-learning based on elevation patterns for road scene interpretation</papertitle>
    </a>
    <br>
    A. Hern√°ndez, <strong>Suhan Woo</strong>, H. Corrales, I. Parra, <a href="https://cilab.yonsei.ac.kr/">Euntai Kim</a>, D. F. Llorca
    <br>
    <em>2020 IEEE Intelligent Vehicles Symposium (IV)</em>
    <br>
    <a href="https://arxiv.org/abs/2009.00330">Paper</a>
    /
    <a href="data/bib/3d_deep_2020.txt">bib</a>
    <p></p>
    <p>
      We propose 3D-DEEP, a deep-learning architecture designed for road scene understanding using elevation patterns derived from disparity-filtered and LiDAR-projected images.
    </p>
  </td>
</tr>


</tbody></table>

				
      
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            This website's source code is borrowed from <a href="https://github.com/jonbarron/jonbarron_website">jonbarron's website</a>.
          </p>
          <p style="text-align:right;font-size:small;">
            Last updated August 2025.
          </p>
        </td>
      </tr>
    </tbody>
  </table>

</body>

</html>
